### Проблема:

- сейчас каждый клиент подключаясь к api напрямую нагружает его polling запросами
- при этом данные для каждого клиента могут отличаться друг от друга, потому что polling не дает синхранизации
- избыточный трафик - клиент получает сырые данные и каждое обновление их фильтрует
- долгая первичная загрузка ( FCP ) - проблема вытекающая из предыдущего пункта, чем меньше вес пакета, тем быстрее пользователь сможет увидеть контент и заимодействовать с приложением

### Варианты решения

Ввести промежуточный слой - bff/api gateway, между `Backend API`(и различными сервисами) и фронтендом.

Благодаря этому можно перенести бизнес логику на `bff` , сделав фронтенд простым, а также оптимизировать запросы, улучшив пользовательский опыт.

Первые шаги:
Вынести логику связанную с обработкой пулов и рейтов на сервер.
Обновлять и рассчитывать их на сервере, а клиенту отдавать подготовленные данные через websocket или sse соеденение

### Списки самих пулов или активов присылать только при инициализации по http, а изменяемые данные (рейты, какие либо поля в пулах), высчитывать на сервере и стримить, и уже на клиенте связывать.

Что сделал

создал MVP `bff` сервера, архитектура - своеобразный DDD. Есть два модуля (assets/pools) они содержат контроллеры, роутер, репозитории и т.п., сервисы вынесены отдельно, есть оч простой ioc для di (можно обойтись без этого).
При инициализации `bff` подключается к `Backend API` для получения данных с помощью сервисов, данные сохраняются в репозиторий модуля. Чтобы отправлять данные на клиент как только они обновились, используется `EventEmitter` (решение считаю узким местом, но это чтобы протестить концепцию)

https://github.com/RuslanShkhaliev/bffddtest/tree/main

### Взаимодействие `bff` - `Backend API`

bff устанавливает соеденение с `Backend API` для получения данных о пулах, по прежднему с помощью polling’a, данные сохраняются в репозиториях модулей с которыми они связаны (планируется, что репозитории отвечают за хранение и обработку данных перед отправкой на клиент)

### Взаимодействие `клиент` - `bff`

Протестировано три подхода, все имеют так или иначе плюсы, как минимум разгружая `API` и уменьшая трафик

`Polling`:

Клиент интервально опрашивает сервер (polling), получает данные которые хранит репозиторий (данные в репо обновляются, но не синхронизированно с опросами клиента).
Этот подход дает меньше всего профита, потому что инициатор запроса клиент и будут гоняться данные, которые могли еще не обновиться, кроме того каждый клиент делает это по своему интервалу и между разыми устройствами данные могут отличаться.

https://github.com/RuslanShkhaliev/bffddtest/tree/pools-sync

`SSE`:

С этим подходом мы инверсируем взаимодействие, теперь клиент получает данные тогда, когда они фактически обновились, при чем каждый клиент видит одни и те же данные. Однако, появляется проблема связанная с тем как работает EventSourcing, а именно накопление сообщений в течении стрима (это увеличивает потребление памяти вкладки), никакой возможности с помощью api браузера фиксануть это я не нашел, но можно интервально закрывать и открывать соеденение, дабы очищать память.
Ну из плюсов, sse при обрыве коннекта, будет пытаться реконнектиться автоматически, в отличии от wss.

https://github.com/RuslanShkhaliev/bffddtest/tree/pools-stream

`Websocket` :https://github.com/RuslanShkhaliev/bffddtest/tree/pools-stream
При инициализации `bff` открываем соеденение, при обновлении данных от `API` шлем сообщение через `WS` , на клиенте при инициализации подключаемся к нужному wss порту и получаем данные.
Самый оптимальный вариант, так как нет болячек `SSE` .

Форс апдейты (запрашиваемые вручную юзерами) по прежднему будут работать, но это стоит кешировать, я этого не касался.

Если говорить комплексно о bff, то плюсов могу перечислить еще больше. Это и вариант взаимодействия между `API` и `bff` и `bff` с `клиентом` , захотели сделали `GQL` , замапили данные как нам удобно, упростили фронт и т.п.

https://github.com/RuslanShkhaliev/bffddtest/tree/pools-wss

https://github.com/RuslanShkhaliev/bffddtest/tree/main
